[TOC]



# 迁移学习研究综述

> 重点是分类当前迁移学习的分类、回归和聚类问题。 本文讨论了迁移学习与领域自适应、多任务学习、样本选择偏差、协变量转移等相关机器学习技术之间的关系 

## 引言

目标：将一个处在源域上的模型用在目标域上的模型上

##  概述

### 迁移学习简史

1. 迁移学习的新职责：系统识别并应用从以前任务中学习到的知识或技能到新任务的能力（旨在从一个或多个源任务中提取知识）
2. 相比于多任务学习，它更加注重于目标任务

### 符号定义

* 源域$$D$$：包含了特征空间 $$\chi$$和边缘分布概率$$P(\chi)$$, 例如$$\chi$$是所有样本的空间，$$\chi_i$$是某一些样本的对应的向量，$$X$$则是一个特定的学习样本。给定一个源域$$D=\big( \chi,P(\chi))$$，每个任务包括两个部分：标签域$$y$$ 和预测函数$$f()$$,每一个训练对象都包括了$$(x_i,y_i)$$。从概率的观点来看$$f(x)$$可以被写为$$P(y|x)$$
* **定义1(迁移学习)**：给定源域$$D_S$$和学习任务$$T_s$$，目标域$$D_T$$ 和学习任务$$T_T$$，迁移学习旨在使用$$D_S$$和$$T_S$$中的知识，帮助优化在$$D_T$$中的预测函数$$f()$$。
* 在上面的定义中$$D_s \neq D_t$$，意味着源域和目标域的特征空间不同或边缘分布概率函数不相同; 当$$T_s \neq T_t$$ 则意味着标签空间不同或条件边缘分布概率不同
* 若源域和目标域的特征空间存在某种显式或隐式的关系，我们则称他们相关。

### 迁移学习技术的分类

1. 三个问题（迁移什么，如何迁移，何时迁移）：

   * 迁移什么

     有些知识在不同域之间可能是通用的

   * 何时迁移

     应当镜像技能转移，有些时候知识不应该被转移（负迁移）
   
2. 三种迁移学习
   ![1618906971406](..\images\1618906971406.png)
   
   1. 在归纳迁移学习设置中，目标任务不同域源任务，此时需从目标域中选取一些标记数据来推断客观预测模型
      * 当源域中有很多数据是可用的时，归纳迁移学习设置类似于多任务学习设置
      * 没有可用数据时，应采用自学设置。
   2. 在转导迁移学习设置中，源任务与目标任务相同，但源域与目标域不同。此时目标域中没有数据可用。此时有两种情况。
      * 源域和目标域的特征空间不同
      * 特征空间相同，但边缘分布概率不同。
   3. 在无监督迁移学习设置中，目标任务与源任务不同但相关。此时没有可用的数据
   
3. 以上三种情况的总结
   ![1618907822531](..\images\1618907822531.png)
   ![1618907926866](..\images\1618907926866.png)

   > 第一种情况可以被称为基于实例的迁移学习，第二种是特征表示转移方法，第三种是参数转移方法，最后一种情况可以被称为关系只是转移方法。

4. 下表是对几类迁移学习方法的总结
   ![1618908245640](..\images\1618908245640.png)
   ![1618908291498](..\images\1618908291498.png)



## 归纳迁移学习 

1. **定义2(归纳迁移学习)**：给定源域 $$D_S$$ 和学习任务 $$T_S$$ ,目标域 $$ D_T $$ 和 $$ T_T $$ ，归纳迁移学习旨在用在 $$D_S $$ 和 $$T_S$$ 上的知识优化在$$ D_T $$上目标检测函数$$ f_T() $$
2. 大多数迁移学习方法倾向于应用于源域中的标记数据可用

### 传递实例的知识

1. `boosting`算法`TrAdaBoost`：假定源域、目标域中使用了完全相同的数据，但分布不同。它试图修改源域数据的加权，鼓励“好”数据，减少“坏”数据的影响；误差仅在目标数据上计算

### 传递特征表示的知识

找到好的 特征表示可以最小化领域误差和分类/回归模型误差。对于不同的源域数据，寻找特征表示的策略不同。

1. 当有大量标记数据可用时，使用监督学习方法来构建特征表示
2. 若没有，则使用无监督学习方法。

#### 有监督特征构建

这里用到的方法类似于多任务学习中使用的方法。

* 基本思想：学习一个可以在相关任务里共享的低维表示。

* 性的表示方法可以减少每个任务的分类或回归模型误差

* `Argyrious`等人提出了一种多任务学习的稀疏特征学习方法，在归纳迁移学习设置中，可以通过一个优化问题来学习常见特征
  
  
     $$
     \begin{aligned}
  &{arg \min}_{A,U} \sum\limits_{t\in\{T,S\}} \sum\limits_{i=1}^{n_t} {L(y_{t_i},\langle a_t,U^Tx_{t_i}\rangle)+\gamma {||A||}_{2,1}^2} \\ 
  & s.t. \qquad U \in {O}^d 
  \end{aligned}
  $$
  
  > A是一个参数矩阵，U是一个d*d的映射函数，用于将原始高位数据低维化；$${||A||}_{r,p}:=(\sum\limits_{i=1}^d{||a^i||}_t^p)^{\frac{1}{p}}$$；优化问题（1）可以转化为等价的凸优化公式
  
* 后续的工作

  * 提出了多任务结构学习的矩阵正则化框架；
  * 提出了一种凸优化算法，从预测任务的集合中同时学习元先验和特征权重；
  * 提出了用SVMs选择多任务学习的特征
  * 设计了一种基于内核的归纳传输方法

#### 无监督特征构建

* 有人提出了稀疏编码，用以学习高级别的特征。这里有两个步骤：

  1. 通过在源域数据上解决如下的优化问题而产生了高阶基向量$$b=\{b_1,b_2,...,b_s\}$$
     $$
     \begin{aligned}
     \min\limits_{a,b} \sum\limits_{i}||x_{S_i}-\sum\limits_j{a_{S_i}^jb_j}||_2^2+\beta||a_{S_i}||_1\\
     s.t. \qquad ||b_j||_2 \leq 1,\quad \forall j\in1,...,s.
     \end{aligned}
     $$

     > $$a_{S_i}^j$$是输入$$x_{S_i}$$在新的基$$b_j$$下的表示，$$\beta$$是平衡了特征构造项和正则化项的系数.

  2. 学习完基向量 $$b_i$$ 后，就可以对目标数据应用下列优化算法，让 $$b_i$$ 学习到更高级别的特征
     $$
     a_{T_i}^*=\arg \min\limits_{a_{T_i}}\lVert x_{T_i}-\sum\limits_ja_{T_i}^jb_j\rVert_2^2+\beta\|a_{T_i}\|_1.
     $$

  3. 最后，鉴别算法就可以应用到具有相应标签的$$\{a_{T_i}^*\}^{'}$$,用来训练分类或或回归模型用于目标域，但在源域上训练出的基向量可能并不适合在目标域中使用

* 有人提出了一种基于普罗科斯特斯分析的流形对齐方法。通过对齐的流形进行知识的跨域传递

### 传递参数知识

* 本节描述的方法多为多任务而设计的，它也容易被修改到应用至迁移学习。我们应当在目标域上的损失函数分配更大的权重

### 传递关系知识

`i.i.d: 独立同分布`

> 它试图将数据之间的关系从源域转移到目标域。

* TAMAR算法：利用马尔科夫逻辑网络在关系域中传递关系知识。
  * 基础：如果两个域项目关联，则可能存在从源域到目标域连接实体及其关系的映射
  * 两个阶段
    * 基于加权伪对数似然度量（WPLL），构建从源域到目标域的映射
    * 通过FORTE算法(用于修改一阶理论的鬼马逻辑编程（ILP）suanfa )对目标域中的映射结构进行修改

## 转导迁移学习

要求源任务和目标任务相同，尽管域可能不同。进一步要求未标记数据也是可用的。

转导有几种含义：

* 在传统机器学习中，它要求在训练时看到所有测试数据，且学习到的模型不能被用与未来的数据。
* 在迁移学习的分类中，用来强调任务必须相同，且在目标域中必须有一些未标记的数据可用

**定义3(转导迁移学习)**：给定$$D_S,T_S,D_T,T_T$$，它旨在使用源域上的数据提高在目标域上的预测函数。此外，一些未标记的数据也需要被用到。

转导迁移学习主要有两种情况：1）源域和目标域之间的特征空间不同；2）域之间的特征空间相同，但输入数据的边缘分布概率函数不同

### 实例知识的转移

> 机器学习理论 之 经验风险最小化（Empirical Risk Minimization）

### 传递特征表示的知识

* `Blitzer`等人提出了一种结构对应学习（SCL）算法，利用目标域中的未标记数据提取出一些关键特征，以减小域之间的差异。
  * 第一步是在来自两个域的未标记数据上定义一组中枢特征（共m个）
  * 从数据中移除了这些枢轴特征后，将每个枢轴视为新的标签向量，可以构造m分类问题，如下所示
    $$f_l(x)=sgn(w_l^T \cdot x),\quad l=1,...,m.$$
    SCI可以学习矩阵$$W=[w_1 w_2...w_m]$$中的参数
  * 将奇异值分解应用于矩阵W
* SCL可以减少域之间的差异，但如何选择中枢特征是困难的，并取决于领域
* 建议使用互信息来选择枢轴特征，而不是使用更多的启发式标准

## 无监督迁移学习

**定义4(无监督迁移学习)**：给定$$T_S,D_S,D_T,T_T$$,它的目的与前几个定义中的目的类似，$$T_S \neq T_T$$ 且$$y_S$$和$$y_T$$都是不可观测的

自学聚类（STC）和转移判别分析（TDA）算法分别提出来转移聚类和转移降维问题。

### 迁移特征表示的知识

`dai`等人研究了一种自学聚类（STC），借助源域中大量未标记数据，对目标域中的少量未标记数据进行聚类。它的目标函数如下表示
![1619009110197](../images/1619009110197.png)

> Z表示$$X_S$$和 $$X_T$$的共享特征空间，I表示两个变量之间的互信息，$$\vec{X_T}$$是$$X_T$$ 对应的簇

我们则通过一以下的优化函数学习$$\vec{X_T}$$
![1619009611933](../images/1619009611933.png)

`Wang`等人提出了一种TDA算法来解决转移位数约减问题。它首先应用聚类方法为目标未标记数据生成伪标签，然后在对目标数据和有标记的源数据进行降维。两个步骤迭代进行，使得目标数据找到最佳子空间

## 转移界限和负转移

我们需要认识到迁移学习的局限性

* `mahmud` 和 `Ray`使用 Kolmogorov 复杂度分析了迁移学习中的情况，并使用它来度量任务之间的相关性
* `Eaton`提出了通过将学习的源模型映射到图中，让源任务之间的关系来度量可迁移性

## 总结

* 未来定义域和任务之间的可迁移性，还需定义度量域或任务之间相似性的标准



