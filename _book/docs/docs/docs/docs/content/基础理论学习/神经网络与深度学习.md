# 神经网络与深度学习

## 使用神经网络识别手写文字

### 感知器

⼀个感知器接受⼏个⼆进制输⼊，并产⽣⼀个⼆进制输出：

<img src="D:/GitBook/images/1621929034268.png" alt="1621929034268" style="zoom: 50%;" />

Rosenblatt 提议⼀个简单的规则来计算输出。他引⼊权重，$$w_1,w_2,...,$$，表⽰相应输⼊对于输出重要性的实数。神经元的输出，0 或者1，则由分配权重后的总和$$\sum_jw_jx_j$$ ⼩于或者⼤于⼀些阈值决定。和权重⼀样，阈值是⼀个实数，⼀个神经元的参数。⽤更精确的代数形式：
$$
output =
\begin{cases} 
0,  & \text{if }\sum_jw_jx_j\le\text{ threshold} \\
1, & \text{if }\sum_jw_jx_j > \text{ threshold}
\end{cases}
$$

### S型神经元

如果对权重（或者偏置）的微⼩的改动真的能够仅仅引起输出的微⼩变化，那我们可以利⽤这⼀事实来修改权重和偏置，让我们的⽹络能够表现得像我们想要的那样。例如，假设⽹络错误地把⼀个“9”的图像分类为“8”。我们能够计算出怎么对<u>**权重和偏置**</u>做些⼩的改动，这样⽹络能够接近于把图像分类为“9”。然后我们要重复这个⼯作，反复改动权重和偏置来产⽣更好的输出。这时⽹络就在<u>学习</u>。

S 型神经元对每个输⼊有权重，$$w_1,w_2,...,$$，和⼀个总的偏置，b。但是输出不是0 或1。相反，它现在是$$\sigma(w \cdot x+b)$$，这⾥$$\sigma$$被称为S 型函数，定义为：

<img src="D:/GitBook/images/1621930438992.png" alt="1621930438992" style="zoom: 50%;" />

把它们放在⼀起来更清楚地说明，⼀个具有输⼊$$x_1,x_2,...,$$，权重$$w_1,w_2,...,$$，和偏置b 的S型神经元的输出是：

<img src="D:/GitBook/images/1621930542085.png" alt="1621930542085" style="zoom:50%;" />

$$\sigma$$绘制的形状是：

<img src="D:/GitBook/images/1621930647451.png" alt="1621930647451" style="zoom:50%;" />

$$\sigma$$的平滑意味着权重和偏置的微⼩变化，即$$\Delta w_j$$ 和Δb，会从神经元产⽣⼀个微⼩的输出变化$$\Delta output$$。实际上，微积分告诉我们Δoutput 可以很好地近似表⽰为：

<img src="D:/GitBook/images/1621931083670.png" alt="1621931083670" style="zoom:50%;" />

### 神经网络的架构

<img src="D:/GitBook/images/1621931541652.png" alt="1621931541652" style="zoom:50%;" />

这个⽹络中最左边的称为输⼊层，其中的神经元称为输⼊神经元。最右边的，即
输出层包含有输出神经元，在本例中，输出层只有⼀个神经元。中间层，既然这层中的神经元既不是输⼊也不是输出，则被称为隐藏层。