{"./":{"url":"./","title":"Introduction","keywords":"","body":"powered by GitbookFile Modify: 2021-05-29 10:24:17 "},"content/":{"url":"content/","title":"基础介绍","keywords":"","body":"1. IntroductionTreeviewCopyright © aleen42 all right reserved, powered by aleen42 Introduction 这是一本书 1. Introduction 这是一本书 powered by GitbookFile Modify: 2021-04-20 10:57:39 "},"content/基础理论学习/":{"url":"content/基础理论学习/","title":"1.基础理论学习","keywords":"","body":"powered by GitbookFile Modify: 2021-05-29 14:09:03 "},"content/基础理论学习/AI算法工程师手册.html":{"url":"content/基础理论学习/AI算法工程师手册.html","title":"AI算法工程师手册","keywords":"","body":"1. 《AI算法工程师手册》1.1. 数学基础1.1.1. 线性代数TreeviewCopyright © aleen42 all right reserved, powered by aleen42 数学基础 线性代数 基本知识 向量操作 1. 《AI算法工程师手册》 1.1. 数学基础 1.1.1. 线性代数 基本知识 矩阵的F范数：设矩阵A=(ai,j)m×nA=(a_{i,j})_{m\\times n}A=(a​i,j​​)​m×n​​ ，则其F 范数为： ∣∣A∣∣F=∑i,jai,j2||A||_F=\\sqrt{\\sum_{i,j}a_{i,j}^2}∣∣A∣∣​F​​=√​∑​i,j​​a​i,j​2​​​​​ 迹的性质 AAA的F 范数等于AATAA^TAA​T​​ 的迹的平方根 AAA的迹等于ATA^TA​T​​ 的迹 tr(AB)=tr(BA)tr(AB)=tr(BA)tr(AB)=tr(BA) 向量操作 三维向量的叉积 w⃗=u⃗×v⃗=[i⃗j⃗k⃗uxuyuzvxvyvz]\r \\vec{w}=\\vec{u}\\times\\vec{v}=\\left[\r \\begin{matrix}\r \\vec{i} & \\vec{j} & \\vec{k} \\\\\r u_x & u_y & u_z \\\\\r v_x & v_y & v_z\r \\end{matrix}\r \\right]\r ​w​⃗​​=​u​⃗​​×​v​⃗​​=​⎣​⎡​​​​i​⃗​​​u​x​​​v​x​​​​​​j​⃗​​​u​y​​​v​y​​​​​​k​⃗​​​u​z​​​v​z​​​​​⎦​⎤​​ powered by GitbookFile Modify: 2021-05-29 13:50:51 "},"content/基础理论学习/神经网络与深度学习.html":{"url":"content/基础理论学习/神经网络与深度学习.html","title":"神经网络与深度学习","keywords":"","body":"1.1. 使用神经网络识别手写文字1.1.1. 感知器1.1.2. S型神经元1.1.3. 神经网络的架构TreeviewCopyright © aleen42 all right reserved, powered by aleen42 使用神经网络识别手写文字 感知器 S型神经元 神经网络的架构 神经网络与深度学习 1.1. 使用神经网络识别手写文字 1.1.1. 感知器 ⼀个感知器接受⼏个⼆进制输⼊，并产⽣⼀个⼆进制输出： Rosenblatt 提议⼀个简单的规则来计算输出。他引⼊权重，w1,w2,...,w_1,w_2,...,w​1​​,w​2​​,...,，表⽰相应输⼊对于输出重要性的实数。神经元的输出，0 或者1，则由分配权重后的总和∑jwjxj\\sum_jw_jx_j∑​j​​w​j​​x​j​​ ⼩于或者⼤于⼀些阈值决定。和权重⼀样，阈值是⼀个实数，⼀个神经元的参数。⽤更精确的代数形式： output={0,if ∑jwjxj≤ threshold1,if ∑jwjxj> threshold\r output =\r \\begin{cases} \r 0, & \\text{if }\\sum_jw_jx_j\\le\\text{ threshold} \\\\\r 1, & \\text{if }\\sum_jw_jx_j > \\text{ threshold}\r \\end{cases}\r output={​0,​1,​​​if ∑​j​​w​j​​x​j​​≤ threshold​if ∑​j​​w​j​​x​j​​> threshold​​ 1.1.2. S型神经元 如果对权重（或者偏置）的微⼩的改动真的能够仅仅引起输出的微⼩变化，那我们可以利⽤这⼀事实来修改权重和偏置，让我们的⽹络能够表现得像我们想要的那样。例如，假设⽹络错误地把⼀个“9”的图像分类为“8”。我们能够计算出怎么对权重和偏置做些⼩的改动，这样⽹络能够接近于把图像分类为“9”。然后我们要重复这个⼯作，反复改动权重和偏置来产⽣更好的输出。这时⽹络就在学习。 S 型神经元对每个输⼊有权重，w1,w2,...,w_1,w_2,...,w​1​​,w​2​​,...,，和⼀个总的偏置，b。但是输出不是0 或1。相反，它现在是σ(w⋅x+b)\\sigma(w \\cdot x+b)σ(w⋅x+b)，这⾥σ\\sigmaσ被称为S 型函数，定义为： 把它们放在⼀起来更清楚地说明，⼀个具有输⼊x1,x2,...,x_1,x_2,...,x​1​​,x​2​​,...,，权重w1,w2,...,w_1,w_2,...,w​1​​,w​2​​,...,，和偏置b 的S型神经元的输出是： σ\\sigmaσ绘制的形状是： σ\\sigmaσ的平滑意味着权重和偏置的微⼩变化，即Δwj\\Delta w_jΔw​j​​ 和Δb，会从神经元产⽣⼀个微⼩的输出变化Δoutput\\Delta outputΔoutput。实际上，微积分告诉我们Δoutput 可以很好地近似表⽰为： 1.1.3. 神经网络的架构 这个⽹络中最左边的称为输⼊层，其中的神经元称为输⼊神经元。最右边的，即 输出层包含有输出神经元，在本例中，输出层只有⼀个神经元。中间层，既然这层中的神经元既不是输⼊也不是输出，则被称为隐藏层。 powered by GitbookFile Modify: 2021-05-29 14:20:16 "},"content/基础理论补充/":{"url":"content/基础理论补充/","title":"2.基础理论补充","keywords":"","body":"powered by GitbookFile Modify: 2021-04-21 15:56:36 "},"content/基础理论补充/基础概念.html":{"url":"content/基础理论补充/基础概念.html","title":"基础概念","keywords":"","body":"1. 基础概念1.1. 指示函数1.1.1. 定义1.1.2. 例子TreeviewCopyright © aleen42 all right reserved, powered by aleen42 指示函数 定义 例子 1. 基础概念 1.1. 指示函数 1.1.1. 定义 在集合论中，指示函数是定义在某集合X上的函数，表示其中有哪些元素属于某一子集A。 1.1.2. 例子 集合X的子集AAA 的指示函数是函数1A:X→{0,1}1_A:X\\rightarrow \\{0,1\\}1​A​​:X→{0,1},定义为 AAA的指示函数也记作 1A(x)1_A(x)1​A​​(x)或XA(x)X_A(x)X​A​​(x) powered by GitbookFile Modify: 2021-05-29 14:23:48 "},"content/迁移学习论文研读/":{"url":"content/迁移学习论文研读/","title":"3.迁移学习论文研读","keywords":"","body":" 主要书写一些在阅读论文过程中的心得 powered by GitbookFile Modify: 2021-04-20 16:57:25 "},"content/迁移学习论文研读/A Survey on Transfer Learning.html":{"url":"content/迁移学习论文研读/A Survey on Transfer Learning.html","title":"A Survey on Transfer Learning","keywords":"","body":"1. 迁移学习研究综述1.1. 引言1.2. 概述1.2.1. 迁移学习简史1.2.2. 符号定义1.2.3. 迁移学习技术的分类1.3. 归纳迁移学习1.3.1. 传递实例的知识1.3.2. 传递特征表示的知识1.3.3. 传递参数知识1.3.4. 传递关系知识1.4. 转导迁移学习1.4.1. 实例知识的转移1.4.2. 传递特征表示的知识1.5. 无监督迁移学习1.5.1. 迁移特征表示的知识1.6. 转移界限和负转移1.7. 总结TreeviewCopyright © aleen42 all right reserved, powered by aleen42 迁移学习研究综述 引言 概述 迁移学习简史 符号定义 迁移学习技术的分类 归纳迁移学习 传递实例的知识 传递特征表示的知识 有监督特征构建 无监督特征构建 传递参数知识 传递关系知识 转导迁移学习 实例知识的转移 传递特征表示的知识 无监督迁移学习 迁移特征表示的知识 转移界限和负转移 总结 [TOC] 1. 迁移学习研究综述 重点是分类当前迁移学习的分类、回归和聚类问题。 本文讨论了迁移学习与领域自适应、多任务学习、样本选择偏差、协变量转移等相关机器学习技术之间的关系 1.1. 引言 目标：将一个处在源域上的模型用在目标域上的模型上 1.2. 概述 1.2.1. 迁移学习简史 迁移学习的新职责：系统识别并应用从以前任务中学习到的知识或技能到新任务的能力（旨在从一个或多个源任务中提取知识） 相比于多任务学习，它更加注重于目标任务 1.2.2. 符号定义 源域DDD：包含了特征空间 χ\\chiχ和边缘分布概率P(χ)P(\\chi)P(χ), 例如χ\\chiχ是所有样本的空间，χi\\chi_iχ​i​​是某一些样本的对应的向量，XXX则是一个特定的学习样本。给定一个源域D=(χ,P(χ))D=\\big( \\chi,P(\\chi))D=(χ,P(χ))，每个任务包括两个部分：标签域yyy 和预测函数f()f()f(),每一个训练对象都包括了(xi,yi)(x_i,y_i)(x​i​​,y​i​​)。从概率的观点来看f(x)f(x)f(x)可以被写为P(y∣x)P(y|x)P(y∣x) 定义1(迁移学习)：给定源域DSD_SD​S​​和学习任务TsT_sT​s​​，目标域DTD_TD​T​​ 和学习任务TTT_TT​T​​，迁移学习旨在使用DSD_SD​S​​和TST_ST​S​​中的知识，帮助优化在DTD_TD​T​​中的预测函数f()f()f()。 在上面的定义中Ds≠DtD_s \\neq D_tD​s​​≠D​t​​，意味着源域和目标域的特征空间不同或边缘分布概率函数不相同; 当Ts≠TtT_s \\neq T_tT​s​​≠T​t​​ 则意味着标签空间不同或条件边缘分布概率不同 若源域和目标域的特征空间存在某种显式或隐式的关系，我们则称他们相关。 1.2.3. 迁移学习技术的分类 三个问题（迁移什么，如何迁移，何时迁移）： 迁移什么 有些知识在不同域之间可能是通用的 何时迁移 应当镜像技能转移，有些时候知识不应该被转移（负迁移） 三种迁移学习 在归纳迁移学习设置中，目标任务不同域源任务，此时需从目标域中选取一些标记数据来推断客观预测模型 当源域中有很多数据是可用的时，归纳迁移学习设置类似于多任务学习设置 没有可用数据时，应采用自学设置。 在转导迁移学习设置中，源任务与目标任务相同，但源域与目标域不同。此时目标域中没有数据可用。此时有两种情况。 源域和目标域的特征空间不同 特征空间相同，但边缘分布概率不同。 在无监督迁移学习设置中，目标任务与源任务不同但相关。此时没有可用的数据 以上三种情况的总结 第一种情况可以被称为基于实例的迁移学习，第二种是特征表示转移方法，第三种是参数转移方法，最后一种情况可以被称为关系只是转移方法。 下表是对几类迁移学习方法的总结 1.3. 归纳迁移学习 定义2(归纳迁移学习)：给定源域 DSD_SD​S​​ 和学习任务 TST_ST​S​​ ,目标域 DT D_T D​T​​ 和 TT T_T T​T​​ ，归纳迁移学习旨在用在 DSD_S D​S​​ 和 TST_ST​S​​ 上的知识优化在DT D_T D​T​​上目标检测函数fT() f_T() f​T​​() 大多数迁移学习方法倾向于应用于源域中的标记数据可用 1.3.1. 传递实例的知识 boosting算法TrAdaBoost：假定源域、目标域中使用了完全相同的数据，但分布不同。它试图修改源域数据的加权，鼓励“好”数据，减少“坏”数据的影响；误差仅在目标数据上计算 1.3.2. 传递特征表示的知识 找到好的 特征表示可以最小化领域误差和分类/回归模型误差。对于不同的源域数据，寻找特征表示的策略不同。 当有大量标记数据可用时，使用监督学习方法来构建特征表示 若没有，则使用无监督学习方法。 有监督特征构建 这里用到的方法类似于多任务学习中使用的方法。 基本思想：学习一个可以在相关任务里共享的低维表示。 性的表示方法可以减少每个任务的分类或回归模型误差 Argyrious等人提出了一种多任务学习的稀疏特征学习方法，在归纳迁移学习设置中，可以通过一个优化问题来学习常见特征 argminA,U∑t∈{T,S}∑i=1ntL(yti,⟨at,UTxti⟩)+γ∣∣A∣∣2,12s.t.U∈Od\r \\begin{aligned}\r &{arg \\min}_{A,U} \\sum\\limits_{t\\in\\{T,S\\}} \\sum\\limits_{i=1}^{n_t} {L(y_{t_i},\\langle a_t,U^Tx_{t_i}\\rangle)+\\gamma {||A||}_{2,1}^2} \\\\ \r & s.t. \\qquad U \\in {O}^d \r \\end{aligned}\r ​​​​​argmin​A,U​​​t∈{T,S}​∑​​​i=1​∑​n​t​​​​L(y​t​i​​​​,⟨a​t​​,U​T​​x​t​i​​​​⟩)+γ∣∣A∣∣​2,1​2​​​s.t.U∈O​d​​​​ A是一个参数矩阵，U是一个d*d的映射函数，用于将原始高位数据低维化；∣∣A∣∣r,p:=(∑i=1d∣∣ai∣∣tp)1p{||A||}_{r,p}:=(\\sum\\limits_{i=1}^d{||a^i||}_t^p)^{\\frac{1}{p}}∣∣A∣∣​r,p​​:=(​i=1​∑​d​​∣∣a​i​​∣∣​t​p​​)​​p​​1​​​​；优化问题（1）可以转化为等价的凸优化公式 后续的工作 提出了多任务结构学习的矩阵正则化框架； 提出了一种凸优化算法，从预测任务的集合中同时学习元先验和特征权重； 提出了用SVMs选择多任务学习的特征 设计了一种基于内核的归纳传输方法 无监督特征构建 有人提出了稀疏编码，用以学习高级别的特征。这里有两个步骤： 通过在源域数据上解决如下的优化问题而产生了高阶基向量b={b1,b2,...,bs}b=\\{b_1,b_2,...,b_s\\}b={b​1​​,b​2​​,...,b​s​​} mina,b∑i∣∣xSi−∑jaSijbj∣∣22+β∣∣aSi∣∣1s.t.∣∣bj∣∣2≤1,∀j∈1,...,s.\r \\begin{aligned}\r \\min\\limits_{a,b} \\sum\\limits_{i}||x_{S_i}-\\sum\\limits_j{a_{S_i}^jb_j}||_2^2+\\beta||a_{S_i}||_1\\\\\r s.t. \\qquad ||b_j||_2 \\leq 1,\\quad \\forall j\\in1,...,s.\r \\end{aligned}\r ​​a,b​min​​​i​∑​​∣∣x​S​i​​​​−​j​∑​​a​S​i​​​j​​b​j​​∣∣​2​2​​+β∣∣a​S​i​​​​∣∣​1​​​s.t.∣∣b​j​​∣∣​2​​≤1,∀j∈1,...,s.​​ aSija_{S_i}^ja​S​i​​​j​​是输入xSix_{S_i}x​S​i​​​​在新的基bjb_jb​j​​下的表示，β\\betaβ是平衡了特征构造项和正则化项的系数. 学习完基向量 bib_ib​i​​ 后，就可以对目标数据应用下列优化算法，让 bib_ib​i​​ 学习到更高级别的特征 aTi∗=argminaTi∥xTi−∑jaTijbj∥22+β∥aTi∥1.\r a_{T_i}^*=\\arg \\min\\limits_{a_{T_i}}\\lVert x_{T_i}-\\sum\\limits_ja_{T_i}^jb_j\\rVert_2^2+\\beta\\|a_{T_i}\\|_1.\r a​T​i​​​∗​​=arg​a​T​i​​​​​min​​∥x​T​i​​​​−​j​∑​​a​T​i​​​j​​b​j​​∥​2​2​​+β∥a​T​i​​​​∥​1​​. 最后，鉴别算法就可以应用到具有相应标签的{aTi∗}′\\{a_{T_i}^*\\}^{'}{a​T​i​​​∗​​}​​′​​​​,用来训练分类或或回归模型用于目标域，但在源域上训练出的基向量可能并不适合在目标域中使用 有人提出了一种基于普罗科斯特斯分析的流形对齐方法。通过对齐的流形进行知识的跨域传递 1.3.3. 传递参数知识 本节描述的方法多为多任务而设计的，它也容易被修改到应用至迁移学习。我们应当在目标域上的损失函数分配更大的权重 1.3.4. 传递关系知识 i.i.d: 独立同分布 它试图将数据之间的关系从源域转移到目标域。 TAMAR算法：利用马尔科夫逻辑网络在关系域中传递关系知识。 基础：如果两个域项目关联，则可能存在从源域到目标域连接实体及其关系的映射 两个阶段 基于加权伪对数似然度量（WPLL），构建从源域到目标域的映射 通过FORTE算法(用于修改一阶理论的鬼马逻辑编程（ILP）suanfa )对目标域中的映射结构进行修改 1.4. 转导迁移学习 要求源任务和目标任务相同，尽管域可能不同。进一步要求未标记数据也是可用的。 转导有几种含义： 在传统机器学习中，它要求在训练时看到所有测试数据，且学习到的模型不能被用与未来的数据。 在迁移学习的分类中，用来强调任务必须相同，且在目标域中必须有一些未标记的数据可用 定义3(转导迁移学习)：给定DS,TS,DT,TTD_S,T_S,D_T,T_TD​S​​,T​S​​,D​T​​,T​T​​，它旨在使用源域上的数据提高在目标域上的预测函数。此外，一些未标记的数据也需要被用到。 转导迁移学习主要有两种情况：1）源域和目标域之间的特征空间不同；2）域之间的特征空间相同，但输入数据的边缘分布概率函数不同 1.4.1. 实例知识的转移 机器学习理论 之 经验风险最小化（Empirical Risk Minimization） 1.4.2. 传递特征表示的知识 Blitzer等人提出了一种结构对应学习（SCL）算法，利用目标域中的未标记数据提取出一些关键特征，以减小域之间的差异。 第一步是在来自两个域的未标记数据上定义一组中枢特征（共m个） 从数据中移除了这些枢轴特征后，将每个枢轴视为新的标签向量，可以构造m分类问题，如下所示 fl(x)=sgn(wlT⋅x),l=1,...,m.f_l(x)=sgn(w_l^T \\cdot x),\\quad l=1,...,m.f​l​​(x)=sgn(w​l​T​​⋅x),l=1,...,m. SCI可以学习矩阵W=[w1w2...wm]W=[w_1 w_2...w_m]W=[w​1​​w​2​​...w​m​​]中的参数 将奇异值分解应用于矩阵W SCL可以减少域之间的差异，但如何选择中枢特征是困难的，并取决于领域 建议使用互信息来选择枢轴特征，而不是使用更多的启发式标准 1.5. 无监督迁移学习 定义4(无监督迁移学习)：给定TS,DS,DT,TTT_S,D_S,D_T,T_TT​S​​,D​S​​,D​T​​,T​T​​,它的目的与前几个定义中的目的类似，TS≠TTT_S \\neq T_TT​S​​≠T​T​​ 且ySy_Sy​S​​和yTy_Ty​T​​都是不可观测的 自学聚类（STC）和转移判别分析（TDA）算法分别提出来转移聚类和转移降维问题。 1.5.1. 迁移特征表示的知识 dai等人研究了一种自学聚类（STC），借助源域中大量未标记数据，对目标域中的少量未标记数据进行聚类。它的目标函数如下表示 Z表示XSX_SX​S​​和 XTX_TX​T​​的共享特征空间，I表示两个变量之间的互信息，XT⃗\\vec{X_T}​X​T​​​⃗​​是XTX_TX​T​​ 对应的簇 我们则通过一以下的优化函数学习XT⃗\\vec{X_T}​X​T​​​⃗​​ Wang等人提出了一种TDA算法来解决转移位数约减问题。它首先应用聚类方法为目标未标记数据生成伪标签，然后在对目标数据和有标记的源数据进行降维。两个步骤迭代进行，使得目标数据找到最佳子空间 1.6. 转移界限和负转移 我们需要认识到迁移学习的局限性 mahmud 和 Ray使用 Kolmogorov 复杂度分析了迁移学习中的情况，并使用它来度量任务之间的相关性 Eaton提出了通过将学习的源模型映射到图中，让源任务之间的关系来度量可迁移性 1.7. 总结 未来定义域和任务之间的可迁移性，还需定义度量域或任务之间相似性的标准 powered by GitbookFile Modify: 2021-05-29 11:27:48 "},"content/其他补充/":{"url":"content/其他补充/","title":"4.其他补充","keywords":"","body":"powered by GitbookFile Modify: 2021-05-29 13:42:45 "}}